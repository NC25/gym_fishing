{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN w/ Monte Carlo Off-Policy Evaluation .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NC25/gym_fishing/blob/master/custom_models/dqn_monte_carlo_off_policy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ZT5XvSp2jD"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj8H7uHXp75a"
      },
      "source": [
        "!git clone https://github.com/boettiger-lab/gym_fishing.git\n",
        "!python gym_fishing/setup.py sdist bdist_wheel\n",
        "!pip install -e ./gym_fishing/\n",
        "!ls\n",
        "!cd gym_fishing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No5hk3f2p-fR"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('INFO')\n",
        "tf.autograph.set_verbosity(0)\n",
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hlv785oXqCIx"
      },
      "source": [
        "import gym_fishing\n",
        "import pandas as pd\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import PPO2\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines import DQN\n",
        "from stable_baselines.deepq.policies import MlpPolicy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdX8tb1pqENR"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIkkH39SqHhc"
      },
      "source": [
        "env = gym.make('fishing-v0')\n",
        "model = DQN(MlpPolicy, env , verbose=2)\n",
        "trained_model = model.learn(total_timesteps=10000)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxois0ujqvgf"
      },
      "source": [
        "from pylab import rcParams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTN5ZYGrq7UU"
      },
      "source": [
        "# Add Monte Carlo Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wji3tr3es4wz"
      },
      "source": [
        "def mc_control_epsilon_greedy(env, num_episodes, discount=.99):\n",
        "\n",
        "  \"\"\"\n",
        "  Monte Carlo Prediction - Estimates value function through sampling\n",
        "  \"\"\"\n",
        "\n",
        "  #set variables as dictionaries with type float\n",
        "  \n",
        "  return_sum = defaultdict(float)\n",
        "  #return_occurence = defaultdict(float)\n",
        "\n",
        "  Value = defaultdict(float)\n",
        "\n",
        "  for i in range(1, num_episodes + 1):\n",
        "    if i % 1 ==0:\n",
        "      print(\"\\rEpisode {}/{}\".format(i, num_episodes), end=\"\") #carriage return\n",
        "            \n",
        "\n",
        "#Generate episode with (state, action, reward) tuple\n",
        "\n",
        "    episode = []\n",
        "    obs = env.reset()\n",
        "    for i in range(num_episodes):\n",
        "      action, _states = trained_model.predict(obs)\n",
        "      next_obs, reward, done, info = env.step(action)\n",
        "      episode.append((obs, action, reward))\n",
        "      if done:\n",
        "        break\n",
        "      obs = next_obs \n",
        "    \n",
        "    #convert to tuple\n",
        "    tuple(episode)\n",
        "    for x in episode:\n",
        "      all_episode_obs = set([tuple(x[0])])\n",
        "      for i in range(obs):\n",
        "        if x[0] == obs:\n",
        "          first_occurence = x in enumerate(episode)\n",
        "\n",
        "     # first_occurence = next(i for i, x in enumerate(episode) if x[0] == obs)\n",
        "      #obtain cumulative return\n",
        "      reward_sum = sum([x[2] * (discount**i) for i, x in enumerate(episode[first_occurence:])])\n",
        "\n",
        "      return_sum[tuple(obs)] += reward_sum\n",
        "      return_occurence = 0\n",
        "      while x <= num_episodes:\n",
        "        return_occurence += 1\n",
        "        if return_occurence == num_episodes:\n",
        "          final\n",
        "\n",
        "      return_occurence += 1.0\n",
        "\n",
        "      Value[tuple(obs)] = return_sum[tuple(obs)] / return_occurence\n",
        "    \n",
        "  return Value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roojNSNH4U4E"
      },
      "source": [
        "prediction = estimate(env, num_episodes=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmbA7ICbD5rd"
      },
      "source": [
        "\n",
        "def weighted_sampling(env, num_episodes, discount = .99):\n",
        "    \"\"\"\n",
        "    Monte Carlo Control Off-Policy Control using Weights for Sampling.\n",
        "    Finds an optimal greedy policy.\n",
        "    \"\"\"\n",
        "    \n",
        "    # creates Q dictionary that maps obs to action values\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space))\n",
        "    #dictionary for weights\n",
        "    C = defaultdict(lambda: np.zeros(env.action_space))\n",
        "    \n",
        "    # learn greey policy\n",
        "    target_policy = env.step(Q)\n",
        "        \n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        if i_episode % 1 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
        "\n",
        "        # Generate an episode to be tuple (state, action, reward) tuples\n",
        "        episode = []\n",
        "        obs = env.reset()\n",
        "        for t in range(100):\n",
        "            # Sample an action from our policy\n",
        "            action, _states = trained_model.predict(obs)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            if done:\n",
        "                break\n",
        "            obs = next_obs\n",
        "        \n",
        "        # Sum of discounted returns\n",
        "        G = 0.0\n",
        "        # weights for return\n",
        "        W = 1.0\n",
        "        for t in range(len(episode))[::-1]:\n",
        "            obs, action, reward = episode[t]\n",
        "            G = discount * G + reward\n",
        "            #  Add weights\n",
        "            C[obs][action] += W\n",
        "            # Update policy\n",
        "            Q[obs][action] += (W / C[obs][action]) * (G - Q[obs][action]\n",
        "                                                      \n",
        "            if action !=  np.argmax(target_policy(obs)):\n",
        "                break\n",
        "            W = W * 1./behavior_policy(obs)[action]\n",
        "        \n",
        "    return Q, target_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UROkH6sKEhN_"
      },
      "source": [
        "Q, policy = mc_control_importance_sampling(env, num_episodes=500000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}