{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "#1  Model-Based Dynamic Programming.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN0fxuknzoFssuE2ZsWFL3y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NC25/gym_fishing/blob/master/abstract_models/model_based_dynamic_programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpSgObFOxDVo"
      },
      "source": [
        "Dynamic Programming breaks down an objective in subparts and solves those subparts to find a total solution\n",
        "\n",
        "Assume that our model is perfect\n",
        "\n",
        "Features:\n",
        "\n",
        "\n",
        "*   Updates policty based on Bellman's Equation\n",
        "*   Objective: Improve the policy to make it greedy\n",
        "*   Policy iteration - iterates evalution and improvement to get optimal policu\n",
        "*   Value iteration\n",
        "*   Generalized policy iteration \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaHwclEodlct"
      },
      "source": [
        "# Policy Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiZOy8Wk1rq-"
      },
      "source": [
        "!git clone https://github.com/dennybritz/reinforcement-learning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSaAriFk_XDK"
      },
      "source": [
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQhCSspY-5Xo"
      },
      "source": [
        "%load gridworld.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXkMJTwpt2Xu"
      },
      "source": [
        "import numpy as np\n",
        "import pprint\n",
        "import sys\n",
        "if \"../\" not in sys.path:\n",
        "  sys.path.append(\"../\") \n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "path = '/content/gridworld.py'\n",
        "\n",
        "from gridworld import GridworldEnv\n",
        "import plotting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RSwqbxF0DwM"
      },
      "source": [
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "env = GridworldEnv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADaOjYGnO-Kh"
      },
      "source": [
        "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
        "\n",
        "\n",
        "    # Set value function as a set of 0s\n",
        "    Value = np.zeros(env.nS)\n",
        "    while True:\n",
        "        delta = 0 #delta is the gradient propogator\n",
        "        #make mini v values\n",
        "        for s in range(env.nS):\n",
        "            v = 0\n",
        "            # Look at the possible next actions\n",
        "            for a, action_prob in enumerate(policy[s]):\n",
        "                # For each action, look at the next possible states\n",
        "                for  prob, next_state, reward, done in env.P[s][a]:\n",
        "                    # Calculate expected value\n",
        "                    v += action_prob * prob * (reward + discount_factor * Value[next_state])\n",
        "\n",
        "            #v = (s, a , s_t+1)\n",
        "\n",
        "            # Determine variation in value function\n",
        "            delta = max(delta, np.abs(v - Value[s]))\n",
        "            Value[s] = v\n",
        "        # Stop evaluating once our value function change is below a threshold (theta)\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return np.array(Value)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uOrpbIpTnbU"
      },
      "source": [
        "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "v = policy_eval(random_policy, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIQVSfxVdBM2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "92f6cf36-bcb2-44c6-d445-bfabcb98563b"
      },
      "source": [
        "print(\"Value Function\".format(v))\n",
        "print(v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value Function\n",
            "[  0.         -13.99993529 -19.99990698 -21.99989761 -13.99993529\n",
            " -17.9999206  -19.99991379 -19.99991477 -19.99990698 -19.99991379\n",
            " -17.99992725 -13.99994569 -21.99989761 -19.99991477 -13.99994569\n",
            "   0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxAI409idXNn"
      },
      "source": [
        "expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
        "np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_zftUJrdpwZ"
      },
      "source": [
        "# Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHcZMKToL6zf"
      },
      "source": [
        "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
        "\n",
        "  def action_value(state, Value):\n",
        "    #determines value of an action at a different state\n",
        "    current_action = np.zeros(env.nA)\n",
        "    for a in range(env.nA):\n",
        "      for prob, next_state, reward, done in env.P[state][a]:\n",
        "        current_action[a] += prob * (reward + discount_factor * Value[next_state])\n",
        "    return current_action\n",
        "\n",
        "  #start with a random policy\n",
        "  policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "\n",
        "  while True:\n",
        "     Value = policy_eval_fn(random_policy, env)\n",
        "\n",
        "     #Will be set to false when we make a change\n",
        "     policy_stable = True\n",
        "\n",
        "     #For each state, choose the best action\n",
        "\n",
        "     for s in range(env.nS):\n",
        "       chosen_action = np.argmax(policy[s]) #best action is equal the action of the optimal policy\n",
        "\n",
        "       #find best action\n",
        "       sample_action = action_value(s, Value)\n",
        "       best_action = np.argmax(action_value)\n",
        "       #np.argmax returns the index of the max value\n",
        "\n",
        "       #Implement Greedy policy\n",
        "       if chosen_action != best_action:\n",
        "         policy_stable = False\n",
        "       policy[s] = np.eye(env.nA)[best_action]\n",
        "     \n",
        "  if policy_stable == True:\n",
        "      return chosen_action, Value    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wFRE43mT5Yl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "c3242165-a7fa-4dc8-bf83-643f71eae9e9"
      },
      "source": [
        "policy, v = policy_improvement(env)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-e28b74ab52cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_improvement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-aa593358bd48>\u001b[0m in \u001b[0;36mpolicy_improvement\u001b[0;34m(env, policy_eval_fn, discount_factor)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m      \u001b[0mValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_eval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m      \u001b[0;31m#Will be set to false when we make a change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-3f3e3bada2df>\u001b[0m in \u001b[0;36mpolicy_eval\u001b[0;34m(policy, env, discount_factor, theta)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mfor\u001b[0m  \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0;31m# Calculate expected value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     \u001b[0mv\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maction_prob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mValue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m#v = (s, a , s_t+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1XIFqIhVqPr"
      },
      "source": [
        "# Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im8buEudbUgr"
      },
      "source": [
        "def value_iteration(env, theta=0.0001,  discount_factor=1.0):\n",
        "\n",
        "  def best_action(state, Value):\n",
        "    action = np.zeroes(env.nA)\n",
        "    for a in range(env.nA):\n",
        "      for prob, next_state, reward, done in env.P[state][a]:\n",
        "        current_action[a] += prob * (reward + discount_factor * Value[s])\n",
        "    return current_action\n",
        "\n",
        "\n",
        "    Value = np.zeros(env.nS)\n",
        "\n",
        "    while True:\n",
        "\n",
        "      delta = 0\n",
        "      for s in range(env.nS):\n",
        "        #find the next best action\n",
        "        sample_action = best_action(state, Value)\n",
        "        best_action = np.max(current_action)\n",
        "        #calculate delta\n",
        "        delta = max(delta, np.abs(best_action - Value[s]))\n",
        "        #update value function\n",
        "        V[s] = best_action\n",
        "      if delta < theta:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "      #implement determinstic policy\n",
        "      policy = np.zeros([env.nS, env.nA])\n",
        "      for s in range(env.nS):\n",
        "        #find best action for state\n",
        "        current_action = best_action(state, Value)\n",
        "        best_action = np.argmax(current_action)\n",
        "        policy[s, best_action] = 1.0\n",
        "\n",
        "      return policy, Value\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eoeporrg6r_"
      },
      "source": [
        "policy, Value = value_iteration(env)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}