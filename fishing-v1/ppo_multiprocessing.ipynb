{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO multiprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NC25/gym_fishing/blob/master/fishing-v1/ppo_multiprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te_84mFmQg5p"
      },
      "source": [
        "# Background\n",
        "\n",
        "Multiprocessing, also known as vectorized environments, is way to make training faster. But with increased speed, there is a cost of reduced accuracy. As a result, it is important to determine the best mix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTMov-RgQl_Q"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH1Srxw4V8vB"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install matplotlib\n",
        "!pip install PILLOW\n",
        "!pip install tf-agents\n",
        "!pip install 'pybullet==2.4.2'\n",
        "!pip install 'pyglet==1.3.2'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install --upgrade setuptools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClRYNMkVvpUX"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wnbz1lJU-aw"
      },
      "source": [
        " !git clone https://github.com/boettiger-lab/gym_fishing.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVzbu5G4VDtO"
      },
      "source": [
        "!python gym_fishing/setup.py sdist bdist_wheel "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lox4H74HVE_s"
      },
      "source": [
        "!pip install -e ./gym_fishing/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaXIaGeAVLB-"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m499cm2eVMl2"
      },
      "source": [
        "!cd gym_fishing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MtUc-FiVPkb"
      },
      "source": [
        "import gym_fishing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJB8YRmYkLl8"
      },
      "source": [
        "### Remove tensorflow warnings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvDDJztVkKvL"
      },
      "source": [
        "# Filter tensorflow version warnings\n",
        "import os\n",
        "# https://stackoverflow.com/questions/40426502/is-there-a-way-to-suppress-the-messages-tensorflow-prints/40426709\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
        "import warnings\n",
        "# https://stackoverflow.com/questions/15777951/how-to-suppress-pandas-future-warning\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('INFO')\n",
        "tf.autograph.set_verbosity(0)\n",
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQunADhw1EXX"
      },
      "source": [
        "# Vectorized Environments\n",
        "\n",
        "Vectorized Environments are for combining multiple environments into a single one. This lets the agent train on multiple environments per step instead of just one environment per step\n",
        "\n",
        "As a result: \n",
        "\n",
        "There agent collects more experience quickly\n",
        "\n",
        "More states will be explored during the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvO5BGrVv2Rk"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines.common import set_global_seeds\n",
        "from stable_baselines import PPO2\n",
        "from stable_baselines import SAC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHslfVkuwALj"
      },
      "source": [
        "from stable_baselines.common.evaluation import evaluate_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWsIT2vP2FzB"
      },
      "source": [
        "## Environment function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S95WiPGwF6z"
      },
      "source": [
        "def make_env(env_id, rank, seed=0):\n",
        "   #rank is the dimensionality of the tensor\n",
        "   \n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        # Important: use a different seed for each environment\n",
        "        env.seed(seed + rank)\n",
        "        return env\n",
        "    set_global_seeds(seed)\n",
        "    return _init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk7Ukbqlbl-i"
      },
      "source": [
        "from stable_baselines.common.cmd_util import make_vec_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h9hrKuyRP7C"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmdNV8UVwTht"
      },
      "source": [
        "env_id = 'fishing-v1'\n",
        "# The different number of processes that will be used\n",
        "PROCESSES_TO_TEST = [1, 2, 4, 8, 16] \n",
        "NUM_EXPERIMENTS = 3 # RL algorithms can often be unstable, so we run several experiments (see https://arxiv.org/abs/1709.06560)\n",
        "TRAIN_STEPS = 5000\n",
        "# Number of episodes for evaluation\n",
        "EVAL_EPS = 20\n",
        "ALGO = PPO2\n",
        "\n",
        "# We will create one environment to evaluate the agent on\n",
        "eval_env = gym.make(env_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y08bJGxj2ezh"
      },
      "source": [
        "## Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhNt_pC8bSSo"
      },
      "source": [
        "reward_averages = []\n",
        "reward_std = []\n",
        "training_times = []\n",
        "total_procs = 0\n",
        "for n_procs in PROCESSES_TO_TEST:\n",
        "    total_procs += n_procs\n",
        "    print('Running for n_procs = {}'.format(n_procs))\n",
        "    if n_procs == 1:\n",
        "        # if there is only one process, there is no need to use multiprocessing\n",
        "        train_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "    else:\n",
        "        # Here we use the \"spawn\" method for launching the processes, more information is available in the doc\n",
        "        # This is equivalent to make_vec_env(env_id, n_envs=n_procs, vec_env_cls=SubprocVecEnv, vec_env_kwargs=dict(start_method='spawn'))\n",
        "        train_env = SubprocVecEnv([make_env(env_id, i+total_procs) for i in range(n_procs)], start_method='spawn')\n",
        "\n",
        "    rewards = []\n",
        "    times = []\n",
        "\n",
        "    for experiment in range(NUM_EXPERIMENTS):\n",
        "        # it is recommended to run several experiments due to variability in results\n",
        "        train_env.reset()\n",
        "        model = ALGO('MlpPolicy', train_env, verbose=0)\n",
        "        start = time.time()\n",
        "        model.learn(total_timesteps=TRAIN_STEPS)\n",
        "        times.append(time.time() - start)\n",
        "        mean_reward, _  = evaluate_policy(model, eval_env, n_eval_episodes=EVAL_EPS)\n",
        "        rewards.append(mean_reward)\n",
        "    # Important: when using subprocess, don't forget to close them\n",
        "    # otherwise, you may have memory issues when running a lot of experiments\n",
        "    train_env.close()\n",
        "    reward_averages.append(np.mean(rewards))\n",
        "    reward_std.append(np.std(rewards))\n",
        "    training_times.append(np.mean(times))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIU5fBmB7bk2"
      },
      "source": [
        "training_steps_per_second = [TRAIN_STEPS / t for t in training_times]\n",
        "\n",
        "plt.figure(figsize=(9, 4))\n",
        "plt.subplots_adjust(wspace=0.5)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.errorbar(PROCESSES_TO_TEST, reward_averages, yerr=reward_std, capsize=2)\n",
        "plt.xlabel('Processes')\n",
        "plt.ylabel('Average return')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(PROCESSES_TO_TEST)), training_steps_per_second)\n",
        "plt.xticks(range(len(PROCESSES_TO_TEST)), PROCESSES_TO_TEST)\n",
        "plt.xlabel('Processes')\n",
        "_ = plt.ylabel('Training steps per second')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7a7ZiVw5A11"
      },
      "source": [
        "## Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUBeAZoU7Ycu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu8ZYFQN3yLr"
      },
      "source": [
        "\n",
        "\n",
        "SECONDS_PER_EXPERIMENT = 10\n",
        "reward_averages = []\n",
        "reward_std = []\n",
        "training_times = []\n",
        "total_procs = 0\n",
        "\n",
        "\n",
        "training_steps_per_second = [TRAIN_STEPS / t for t in training_times]\n",
        "\n",
        "steps_per_experiment = [int(SECONDS_PER_EXPERIMENT * fps) for fps in training_steps_per_second]\n",
        "\n",
        "for n_procs, train_steps in zip(PROCESSES_TO_TEST, steps_per_experiment):\n",
        "    total_procs += n_procs\n",
        "    print('Running for n_procs = {} for steps = {}'.format(n_procs, train_steps))\n",
        "    if n_procs == 1:\n",
        "        # if there is only one process, there is no need to use multiprocessing\n",
        "        train_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "    else:\n",
        "        train_env = SubprocVecEnv([make_env(env_id, i+total_procs) for i in range(n_procs)], start_method='spawn')\n",
        "        # Alternatively, you can use a DummyVecEnv if the communication delays is the bottleneck\n",
        "        # train_env = DummyVecEnv([make_env(env_id, i+total_procs) for i in range(n_procs)])\n",
        "\n",
        "    rewards = []\n",
        "    times = []\n",
        "\n",
        "    for experiment in range(NUM_EXPERIMENTS):\n",
        "        # it is recommended to run several experiments due to variability in results\n",
        "        train_env.reset()\n",
        "        model = ALGO('MlpPolicy', train_env, verbose=0)\n",
        "        start = time.time()\n",
        "        model.learn(total_timesteps=train_steps)\n",
        "        times.append(time.time() - start)\n",
        "        mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=EVAL_EPS)\n",
        "        rewards.append(mean_reward)\n",
        "\n",
        "    train_env.close()\n",
        "    reward_averages.append(np.mean(rewards))\n",
        "    reward_std.append(np.std(rewards))\n",
        "    training_times.append(np.mean(times))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQXJ1hI46DVB"
      },
      "source": [
        "training_steps_per_second = [s / t for s,t in zip(steps_per_experiment, training_times)]\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.errorbar(PROCESSES_TO_TEST, reward_averages, yerr=reward_std, capsize=2, c='k', marker='o')\n",
        "plt.xlabel('Processes')\n",
        "plt.ylabel('Average return')\n",
        "plt.subplot(1,2,2)\n",
        "plt.bar(range(len(PROCESSES_TO_TEST)), training_steps_per_second)\n",
        "plt.xticks(range(len(PROCESSES_TO_TEST)),PROCESSES_TO_TEST)\n",
        "plt.xlabel('Processes')\n",
        "plt.ylabel('Training steps per second')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}