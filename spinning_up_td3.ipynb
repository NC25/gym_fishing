{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spinning up AI TD3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6MsumpXvvrgwvnb8Z3/oq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NC25/gym_fishing/blob/master/spinning_up_td3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUyG5YNwtpRF"
      },
      "source": [
        "Steps for Custom algortihims\n",
        "\n",
        "1.   Logger (probility distribution)\n",
        "\n",
        "2.   Set seed\n",
        "\n",
        "1.   Instantiate env\n",
        "\n",
        "1.   Placeholders for computation\n",
        "\n",
        "1.   Experience Buffer\n",
        "2.  Helper Functions\n",
        "\n",
        "7.   Running main loop\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjypK4wHtdGg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "5c29a399-7545-47bc-e527-455d1f50d867"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.4).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: stable-baselines[mpi]==2.10.0 in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.0.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.2)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.6 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o9Rag7Dthx8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "c860904e-9ee5-46f2-8497-b01f8794bf58"
      },
      "source": [
        "!git clone https://github.com/boettiger-lab/gym_fishing.git\n",
        "!python gym_fishing/setup.py sdist bdist_wheel\n",
        "!pip install -e ./gym_fishing/\n",
        "!ls\n",
        "!cd gym_fishing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gym_fishing' already exists and is not an empty directory.\n",
            "running sdist\n",
            "running egg_info\n",
            "writing gym_fishing.egg-info/PKG-INFO\n",
            "writing dependency_links to gym_fishing.egg-info/dependency_links.txt\n",
            "writing top-level names to gym_fishing.egg-info/top_level.txt\n",
            "reading manifest file 'gym_fishing.egg-info/SOURCES.txt'\n",
            "writing manifest file 'gym_fishing.egg-info/SOURCES.txt'\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "running check\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "creating gym_fishing-0.0.2\n",
            "creating gym_fishing-0.0.2/gym_fishing.egg-info\n",
            "copying files to gym_fishing-0.0.2...\n",
            "copying gym_fishing.egg-info/PKG-INFO -> gym_fishing-0.0.2/gym_fishing.egg-info\n",
            "copying gym_fishing.egg-info/SOURCES.txt -> gym_fishing-0.0.2/gym_fishing.egg-info\n",
            "copying gym_fishing.egg-info/dependency_links.txt -> gym_fishing-0.0.2/gym_fishing.egg-info\n",
            "copying gym_fishing.egg-info/top_level.txt -> gym_fishing-0.0.2/gym_fishing.egg-info\n",
            "Writing gym_fishing-0.0.2/setup.cfg\n",
            "Creating tar archive\n",
            "removing 'gym_fishing-0.0.2' (and everything under it)\n",
            "running bdist_wheel\n",
            "running build\n",
            "installing to build/bdist.linux-x86_64/wheel\n",
            "running install\n",
            "running install_egg_info\n",
            "Copying gym_fishing.egg-info to build/bdist.linux-x86_64/wheel/gym_fishing-0.0.2-py3.6.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.linux-x86_64/wheel/gym_fishing-0.0.2.dist-info/WHEEL\n",
            "creating 'dist/gym_fishing-0.0.2-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "adding 'gym_fishing-0.0.2.dist-info/METADATA'\n",
            "adding 'gym_fishing-0.0.2.dist-info/WHEEL'\n",
            "adding 'gym_fishing-0.0.2.dist-info/top_level.txt'\n",
            "adding 'gym_fishing-0.0.2.dist-info/RECORD'\n",
            "removing build/bdist.linux-x86_64/wheel\n",
            "Obtaining file:///content/gym_fishing\n",
            "Installing collected packages: gym-fishing\n",
            "  Found existing installation: gym-fishing 0.0.2\n",
            "    Can't uninstall 'gym-fishing'. No files were found to uninstall.\n",
            "  Running setup.py develop for gym-fishing\n",
            "Successfully installed gym-fishing\n",
            "build  dist  gym_fishing  gym_fishing.egg-info\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaHUEO_Xth0-"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('INFO')\n",
        "tf.autograph.set_verbosity(0)\n",
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OsyRm7HukxW"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines.common.policies import LstmPolicy\n",
        "from stable_baselines import TD3\n",
        "from stable_baselines.td3.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.ddpg.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TRz4SW6uk0q"
      },
      "source": [
        "import gym_fishing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXVQNZCz5GAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3451e0d1-3033-486d-810f-5c97597982ff"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e3-5yfZvpFo"
      },
      "source": [
        "# Stable Baselines: Custom Actor-Critic Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x27rnj0qMiqS"
      },
      "source": [
        "from stable_baselines.common.policies import ActorCriticPolicy, register_policy, nature_cnn\n",
        "from stable_baselines.common.distributions import DiagGaussianProbabilityDistribution\n",
        "\n",
        "\"\"\"\n",
        "    Policy object that implements actor critic\n",
        "\n",
        "    :sess: Current TensorFlow session\n",
        "    :param ob_space: The observation space of env\n",
        "    :param ac_space: (Gym Space) The action space of the environment\n",
        "    :param n_env: (int) The number of environments to run for multiprocessing\n",
        "    :param n_steps: (int) The number of steps to run for each environment\n",
        "    :param n_batch: (int) The number of batchs to run (n_envs * n_steps)\n",
        "    :param reuse: (bool) If the policy is reusable or not\n",
        "    :param scale: (bool) whether or not to scale the input\n",
        "    \"\"\"\n",
        "\n",
        "#Custom MLP Policy: Actor has 3 layers of size 128, Critic has 2 layers of size 32\n",
        "# with a nature_cnn feature extractor\n",
        "\n",
        "class CustomPolicy(ActorCriticPolicy):\n",
        "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **kwargs):\n",
        "        super(CustomPolicy, self).__init__(sess, ob_space, ac_space, n_env=n_env, n_steps=n_steps, n_batch=n_batch, reuse=reuse, scale=True)\n",
        "\n",
        "        self._probability_distribution = DiagGaussianProbabilityDistribution\n",
        "        self._pdtype = make_proba_dist_type(ac_space)\n",
        "        self._policy = None\n",
        "        self._proba_distribution = None\n",
        "        self._value_fn = None\n",
        "        self._action = None\n",
        "        self._deterministic_action = None\n",
        "        self.n_env = 2\n",
        "        self.n_steps = 1000\n",
        "        n_batch = 2000\n",
        "\n",
        "        with tf.variable_scope(\"model\", reuse=reuse):\n",
        "            activation = tf.nn.relu\n",
        "\n",
        "            output = nature_cnn(self.processed_obs, **kwargs)\n",
        "            output = tf.layers.flatten(output)\n",
        "\n",
        "            pi_h = output\n",
        "            for i, layer_size in enumerate([128, 128, 128]):\n",
        "                pi_h = activ(tf.layers.dense(pi_h, layer_size, name='pi_fc' + str(i)))\n",
        "            pi_latent = pi_h\n",
        "\n",
        "            vf_h = output\n",
        "            for i, layer_size in enumerate([32, 32]):\n",
        "                vf_h = activ(tf.layers.dense(vf_h, layer_size, name='vf_fc' + str(i)))\n",
        "            value_fn = tf.layers.dense(vf_h, 1, name='vf')\n",
        "            vf_latent = vf_h\n",
        "\n",
        "            self._proba_distribution, self._policy, self.q_value = \\\n",
        "                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
        "\n",
        "        self._value_fn = value_fn\n",
        "        self._setup_init()\n",
        "\n",
        "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
        "        if deterministic:\n",
        "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
        "                                                   {self.obs_ph: obs})\n",
        "        else:\n",
        "            action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n",
        "                                                   {self.obs_ph: obs})\n",
        "        return action, value, self.initial_state, neglogp\n",
        "\n",
        "    def proba_step(self, obs, state=None, mask=None):\n",
        "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
        "\n",
        "    def value(self, obs, state=None, mask=None):\n",
        "        return self.sess.run(self.value_flat, {self.obs_ph: obs})\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn949xvJSIHJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "4b985bde-e62c-4ca8-a346-ffccf01513e3"
      },
      "source": [
        "env = DummyVecEnv([lambda: gym.make('fishing-v1')])\n",
        "\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "model = TD3(CustomPolicy, env, action_noise=action_noise, verbose=1)\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=100000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-acbc4416992a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maction_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalActionNoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTD3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/td3/td3.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, gamma, learning_rate, buffer_size, learning_starts, train_freq, gradient_steps, batch_size, tau, policy_delay, action_noise, target_policy_noise, target_noise_clip, random_exploration, verbose, tensorboard_log, _init_setup_model, policy_kwargs, full_tensorboard_log, seed, n_cpu_tf_sess)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_pretrain_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/td3/td3.py\u001b[0m in \u001b[0;36msetup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0;31m# Create policy and target TF objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     self.policy_tf = self.policy(self.sess, self.observation_space, self.action_space,\n\u001b[0;32m--> 135\u001b[0;31m                                                  **self.policy_kwargs)\n\u001b[0m\u001b[1;32m    136\u001b[0m                     self.target_policy_tf = self.policy(self.sess, self.observation_space, self.action_space,\n\u001b[1;32m    137\u001b[0m                                                         **self.policy_kwargs)\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'n_env', 'n_steps', and 'n_batch'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG-CZng8SsI-"
      },
      "source": [
        "# Spinning AI: Custom Neural Network and Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxTMsK5Z7zuU"
      },
      "source": [
        "def rnn():\n",
        "  inputs = keras.Input(shape=(env.observation_space.shape))\n",
        "  tf.keras.utils.normalize(inputs, axis=-1, order=2)\n",
        "  layer_1 = layers.Dense(64, activation=\"relu\")\n",
        "  x = layer_1(inputs)\n",
        "  layer_2 = layers.Dense(32, activation=\"relu\")(x)\n",
        "  x = layer_2(inputs)\n",
        "  output = layers.Dense(8, activation='relu') \n",
        "  x = output(inputs)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs, name=None)\n",
        "  inputs.reshape(3, 2)\n",
        "  model.add(layers.LSTM(64))\n",
        "  model.add(layers.Dense(8))\n",
        "  output = model.compile(loss=mean_squared_error, optimizer='adam')\n",
        "  return output\n",
        "\n",
        "rnn\n",
        "#This will already be done when we make the policy or call agent\n",
        "#model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pojVz710u2zx"
      },
      "source": [
        "def get_vars(scope):\n",
        "  return[x for x in tf.global_variables() if scope in x.name]\n",
        "\n",
        "def count_vars(scope):\n",
        "  v = get_vars(scope)\n",
        "  return sum(np.prod(var.shape.as_list()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdDzOCiy17FZ"
      },
      "source": [
        "!git clone https://github.com/openai/spinningup.git\n",
        "!cd spinningup\n",
        "!pip install -e ./spinningup/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2d-HzDl0-Lt"
      },
      "source": [
        "import spinningup\n",
        "from logx import EpochLogger\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jfprpc_3Ddz"
      },
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, obs_dim, act_dim, size):\n",
        "\n",
        "    self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)\n",
        "    self.rewards_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.output_buf = np.zeros(sizem dtype=np.float32)\n",
        "    self.ptr, self.size, self.max_size= 0,0, size\n",
        "\n",
        "    def store(self, obs, act, rew, next_obs, done):\n",
        "        self.obs1_buf[self.ptr] = obs\n",
        "        self.obs2_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rewards_buf[self.ptr] = rew\n",
        "        self.outputs_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "        return dict(obs1=self.obs1_buf[idxs],\n",
        "                    obs2=self.obs2_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rewards_buf[idxs],\n",
        "                    done=self.output_buf[idxs])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmoFR_RF550G"
      },
      "source": [
        "import core"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KbmhDrO5oX5"
      },
      "source": [
        "def td3(env_fn, actor_critic=lstm_actor_critic(), ac_kwargs=dict(), seed=0, \n",
        "        steps_per_epoch=1000, epochs=300, replay_size=int(1e3), gamma=0.99, \n",
        "        polyak=0.995, pi_lr=1e-3, q_lr=1e-3, batch_size=100, start_steps=10000, \n",
        "        update_after=1000, update_every=50, act_noise=0.1, target_noise=0.2, \n",
        "        noise_clip=0.5, policy_delay=2, num_test_episodes=10, max_ep_len=1000, \n",
        "        logger_kwargs=dict(), save_freq=1):\n",
        "  \n",
        "  #polyak: averaging for target networks\n",
        "  #pi_lr: learning rate for policy\n",
        "  #q_lr: learneing rate for Q-networks\n",
        "  #update_every: updates after every number of steps\n",
        "\n",
        "  logger = EpochLogger(**logger_kwargs)\n",
        "  logger.save_config(locals())\n",
        "\n",
        "  tf.set_random_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "#set dimensions\n",
        "  env = env_fn()\n",
        "  observation_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.shape[0]\n",
        "\n",
        "  ac_kwargs['action_space'] = env.action_space\n",
        "  #make all the dimensions the same bound\n",
        "\n",
        "\n",
        "  #Target policy networks\n",
        "  with tf.variable_scope('target'):\n",
        "        pi_targ, _, _, _  = actor_critic(x2_ph, a_ph, **ac_kwargs)\n",
        "    \n",
        "  # Target Q networks\n",
        "  with tf.variable_scope('target', reuse=True):\n",
        "\n",
        "    #add noise\n",
        "    epsilon = tf.random_normal(tf.shape(pi_targ), stddev=target_noise)\n",
        "    epsilon = tf.clip_by_value(epsilon, -noise_clip, noise_clip)\n",
        "    smoothed = pi_targ + epsilon\n",
        "    smoothed = tf.clip_by_value(smoothed, -act_limit, act_limit)\n",
        "\n",
        "    #set target Q values\n",
        "    _, q1_targ, q2_targ, _ = actor_critic(x2_ph, a2, **ac_kwargs)\n",
        "\n",
        "  #Experience Buffer\n",
        "  replay_buffer = ReplayBuffer(observation_dim=observation_dim, action_dim=action_dim, size=replay_size)\n",
        "\n",
        "  #Add Bellman Backup for stability\n",
        "  min_q_target = tf.minimum(q1_target, q2_target)\n",
        "  \n",
        "  backup = tf.stop_gradient(r_ph + gamma*(1-d_p)*min_q_target)\n",
        "\n",
        "  #Loss function for TD3\n",
        "  pi_loss = -tf.reduce_mean(q1_pi)\n",
        "  q1_loss = tf.reduce_mean((q1-backup)**2)\n",
        "  q2_loss = tf.reduce.mean((q2-backup)**2)\n",
        "  q_loss = q1_loss + q2_loss\n",
        "\n",
        "\n",
        "  #optimizer\n",
        "  pi_optimizer = tf.train.AdamOptimizer(learning_rate=pi_lr)\n",
        "  q_optimizer = tf.train.AdamOptimizer(learning_rate=q_lr)\n",
        "  train_pi_op = pi_optimizer.minimize(pi_loss, var_list=get_vars('main/pi'))\n",
        "  train_q_op = q_optimizer.minimize(q_loss, var_list=get_vars('main/q'))\n",
        "\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(target_init)\n",
        "\n",
        "  #save weights\n",
        "  logger.setup_tf_saver(sess, inputs={'x': x_ph, 'a': a_ph}, outputs={'pi': pi, 'q1': q1, 'q2': q2})\n",
        "\n",
        "  def step(observation, noise_scale):\n",
        "  action = sess.run(pi, feed_dict={x_ph: observation.reshape(1, -1)})\n",
        "  action += noise_scale + np.random.randn(action_dim)\n",
        "  return np.clip(a, -act_limit, act_limit)\n",
        "\n",
        "  def sample():\n",
        "  for i in range(num_test_episodes):\n",
        "    observation = env.reset()\n",
        "    while ep_len <= max_ep_len:\n",
        "      observation, reward, done, _ = env.step(action(observation, 0))\n",
        "      ep_return += reward\n",
        "      ep_len += 1\n",
        "      d = False if ep_len==max_ep_len else d\n",
        "      replay_buffer.store(observation, action, reward, observation_2, done)\n",
        "      observation = observation_2\n",
        "    logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfjIvYgUvFtn"
      },
      "source": [
        "from spinup.utils.run_utils import setup_logger_kwargs\n",
        "logger_kwargs = setup_logger_kwargs(args.exp_name, args.seed)\n",
        "\n",
        "\n",
        "td3()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}